{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjy23j15HxoV"
      },
      "source": [
        "\n",
        "From Kaggle: \n",
        "\"MNIST (\"Modified National Institute of Standards and Technology\") is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.\"\n",
        "\n",
        "[Read more.](https://www.kaggle.com/c/digit-recognizer)\n",
        "\n",
        "\n",
        "<a title=\"By Josef Steppan [CC BY-SA 4.0 (https://creativecommons.org/licenses/by-sa/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:MnistExamples.png\"><img width=\"512\" alt=\"MnistExamples\" src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNkrpGYIHxoY"
      },
      "source": [
        "This code is adopted from the pytorch examples repository. \n",
        "It is licensed under BSD 3-Clause \"New\" or \"Revised\" License.\n",
        "Source: https://github.com/pytorch/examples/\n",
        "LICENSE: https://github.com/pytorch/examples/blob/master/LICENSE\n",
        "\n",
        "![](https://github.com/rpi-techfundamentals/fall2018-materials/blob/master/10-deep-learning/mnist-comparison.png?raw=1)\n",
        "Table from [Wikipedia](https://en.wikipedia.org/wiki/MNIST_database)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnlOEtprHxoZ",
        "outputId": "4dc679dc-1844-4b4b-ed9c-64568668323f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvxJmR3sHxof"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnIwYchdHxog"
      },
      "outputs": [],
      "source": [
        "# Set Parameters\n",
        "\n",
        "args={}\n",
        "kwargs={}\n",
        "args['batch_size']=1000\n",
        "args['test_batch_size']=1000\n",
        "args['epochs']=10 #The number of Epochs is the number of times you go through the full dataset. \n",
        "args['lr']=0.048 #Learning rate is how fast it will decend. \n",
        "args['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n",
        "\n",
        "args['seed']=1 #random seed\n",
        "args['log_interval']=10\n",
        "args['cuda']=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2kZoJyCHxoo",
        "outputId": "4787ff83-452a-4070-9034-fba8f310c385",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 89051245.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 110123358.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26534979.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7341244.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the MNIST data\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n27TZ7NTHxos"
      },
      "outputs": [],
      "source": [
        "# Create a Simple CNN model\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    #This defines the structure of the NN.\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()  #Dropout\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Convolutional Layer/Pooling Layer/Activation\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2)) \n",
        "        #Convolutional Layer/Dropout/Pooling Layer/Activation\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        #Fully Connected Layer/Activation\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        #Fully Connected Layer/Activation\n",
        "        x = self.fc2(x)\n",
        "        #Softmax gets probabilities. \n",
        "        return F.log_softmax(x, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tKedBV8Hxot"
      },
      "outputs": [],
      "source": [
        "# Functions to train and to test\n",
        "\n",
        "def train(model, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        #Variables in Pytorch are differenciable. \n",
        "        data, target = Variable(data), Variable(target)\n",
        "        #This will zero out the gradients for this batch. \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
        "        loss = F.nll_loss(output, target)\n",
        "        #dloss/dx for every Variable \n",
        "        loss.backward()\n",
        "        #to do a one-step update on our parameter.\n",
        "        optimizer.step()\n",
        "        #Print out the loss periodically. \n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
        "\n",
        "def test(model):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if args['cuda']:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, reduction='sum').data.item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOmrbF5VHxou",
        "outputId": "9af92ce1-9fa6-4b01-e7a1-a67c9b463391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308819\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.252537\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.076068\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.607672\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.149629\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 1.026792\n",
            "\n",
            "Test set: Average loss: 0.4862, Accuracy: 8758/10000 (88%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.843926\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.825237\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.724626\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.693696\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.711946\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.630045\n",
            "\n",
            "Test set: Average loss: 0.2717, Accuracy: 9192/10000 (92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.544828\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.567881\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.507167\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.454725\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.526988\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.444777\n",
            "\n",
            "Test set: Average loss: 0.1976, Accuracy: 9417/10000 (94%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.439639\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.436341\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.446702\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.429103\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.402771\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.380600\n",
            "\n",
            "Test set: Average loss: 0.1536, Accuracy: 9542/10000 (95%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.417125\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.412153\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.369321\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.379474\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.370240\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.336500\n",
            "\n",
            "Test set: Average loss: 0.1282, Accuracy: 9629/10000 (96%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.398469\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.339055\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.327626\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.382309\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.306365\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.279958\n",
            "\n",
            "Test set: Average loss: 0.1133, Accuracy: 9639/10000 (96%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.297990\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.337018\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.263047\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.311410\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.281836\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.279459\n",
            "\n",
            "Test set: Average loss: 0.1029, Accuracy: 9680/10000 (97%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.293004\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.265417\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.282173\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.283903\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.231970\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.259720\n",
            "\n",
            "Test set: Average loss: 0.0924, Accuracy: 9713/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.283207\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.209623\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.215203\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.284685\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.261420\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.255872\n",
            "\n",
            "Test set: Average loss: 0.0885, Accuracy: 9735/10000 (97%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.272752\n",
            "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.252245\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.252955\n",
            "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.262341\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.252258\n",
            "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.271898\n",
            "\n",
            "Test set: Average loss: 0.0822, Accuracy: 9752/10000 (98%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the model\n",
        "\n",
        "model_simple = SimpleNet()\n",
        "if args['cuda']:\n",
        "    model_simple.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model_simple.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "for epoch in range(1, args['epochs'] + 1):\n",
        "    train(model_simple, epoch)\n",
        "    test(model_simple)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}